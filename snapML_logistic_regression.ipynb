{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Credit Risk Analysis using IBM Snap ML</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore a customer credit history dataset (1 million observations) and train a logistic regression model to predict credit default behavoir. You will need to first download this dataset into the data folder. You can find the dataset at: https://ibm.box.com/s/ithxaw0lx7ccyylrek1sge0v4kajbqfx\n",
    "\n",
    "We will compare the time it takes to train logistic regression with SKLearn and Snap ML on CPU. \n",
    "We're also going to use some common data science libraries, namely Numpy and Pandas for working with data, Matplotlib for visualisations and SKLearn for training our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, normalize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, roc_auc_score\n",
    "from scipy.stats import chi2_contingency,ttest_ind\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import zipfile\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is not in the data folder. Download data from: https://ibm.box.com/s/8r2osyysmrh0tks9dsd3oo501lqh1v0m\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"data/credit_customer_history.csv.zip\"):\n",
    "    print(\"Dataset is not in the data folder. Firstly you should download data from: https://ibm.box.com/s/8r2osyysmrh0tks9dsd3oo501lqh1v0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zipfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-95ce8e10b01d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# unzip dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/credit_customer_history.csv.zip\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zipfile' is not defined"
     ]
    }
   ],
   "source": [
    "# unzip dataset\n",
    "with zipfile.ZipFile(\"data/credit_customer_history.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cust_pd = pd.read_csv('data/credit_customer_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are \" + str(len(cust_pd)) + \" observations in the customer history dataset.\")\n",
    "print(\"There are \" + str(len(cust_pd.columns)) + \" variables in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cust_pd['NUMBER_CREDITS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cust_pd['EMI_TENURE']\n",
    "y = cust_pd['TRANSACTION_AMOUNT']\n",
    "\n",
    "x_bin = len(cust_pd['EMI_TENURE'].unique())\n",
    "y_bin = len(cust_pd['TRANSACTION_AMOUNT'].unique())\n",
    "\n",
    "fig = plt.figure(figsize=(0.3*x_bin, 0.3*y_bin))\n",
    "\n",
    "graph = plt.hist2d(x,y, bins=(x_bin, y_bin))\n",
    "plt.xlabel('EMI_TENURE')\n",
    "plt.ylabel('TRANSACTION_AMOUNT')\n",
    "plt.colorbar(graph[3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cust_pd.groupby(['IS_DEFAULT']).size())\n",
    "index = ['Yes','No']\n",
    "churn_plot = cust_pd['IS_DEFAULT'].value_counts(sort=True, ascending=False).plot(kind='bar',figsize=(4,4),title=\"Total number for occurences of loan default \" + str(cust_pd['IS_DEFAULT'].count()), color=['#BB6B5A','#8CCB9B'])\n",
    "churn_plot.set_xlabel(\"IS_DEFAULT\")\n",
    "churn_plot.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Data preparation is a very important step in machine learning model building. This is because the model can perform well only when the data it is trained on is good and well prepared. Hence, this step consumes bulk of data scientist's time spent building models.\n",
    "\n",
    "During this process, we identify categorical columns in the dataset. Categories needed to be indexed, which means the string labels are converted to label indices. These label indices are encoded using One-hot encoding to a binary vector. This encoding allows algorithms which expect continuous features to use categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into Features (X) and Labels (y)\n",
    "cust_pd_Y = cust_pd[['IS_DEFAULT']]\n",
    "cust_pd_X = cust_pd.drop(['IS_DEFAULT'],axis=1)\n",
    "print('cust_pd_X.shape=', cust_pd_X.shape, 'cust_pd_Y.shape=', cust_pd_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Labels (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "cust_pd_Y[\"IS_DEFAULT\"] = le.fit_transform(cust_pd_Y['IS_DEFAULT'])\n",
    "cust_pd_Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Features (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features X dataframe shape = ', cust_pd_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalColumns = ['CREDIT_HISTORY', 'TRANSACTION_CATEGORY', 'ACCOUNT_TYPE', 'ACCOUNT_AGE',\n",
    "                      'STATE', 'IS_URBAN', 'IS_STATE_BORDER', 'HAS_CO_APPLICANT', 'HAS_GUARANTOR',\n",
    "                      'OWN_REAL_ESTATE', 'OTHER_INSTALMENT_PLAN',\n",
    "                      'OWN_RESIDENCE', 'RFM_SCORE', 'OWN_CAR', 'SHIP_INTERNATIONAL']\n",
    "cust_pd_X = pd.get_dummies(cust_pd_X, columns=categoricalColumns)\n",
    "cust_pd_X.head()\n",
    "\n",
    "print('features X dataframe shape = ', cust_pd_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_pd_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "features = min_max_scaler.fit_transform(cust_pd_X)\n",
    "features = normalize(features, axis=1, norm='l1')\n",
    "\n",
    "cust_pd_X = pd.DataFrame(features,columns=cust_pd_X.columns)\n",
    "cust_pd_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels    = cust_pd_Y.values\n",
    "features  = cust_pd_X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.reshape(labels,(-1,1))\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(features, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "                    \n",
    "print('X_train.shape=', X_train.shape, 'Y_train.shape=', y_train.shape)\n",
    "print('X_test.shape=', X_test.shape, 'Y_test.shape=', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Logistic Regression Model using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sklearn_lr = LogisticRegression()\n",
    "\n",
    "# from pai4sk.linear_model import LogisticRegression #or you can import it directly from snapml library and choose a scikit-learn solver, e.g.liblinear\n",
    "# sklearn_lr = LogisticRegression(solver='liblinear') \n",
    "print(sklearn_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model using Scikit-Learn\n",
    "t0 = time.time()\n",
    "sklearn_lr.fit(X_train, y_train)\n",
    "sklearn_time = time.time() - t0\n",
    "print(\"[sklearn] Training time (s):  {0:.2f}\".format(sklearn_time))\n",
    "\n",
    "# Evaluate accuracy on test set\n",
    "sklearn_pred = sklearn_lr.predict(X_test)\n",
    "print('[sklearn] Accuracy score : {0:.6f}'.format(accuracy_score(y_test, sklearn_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Logistic Regression Model using Snap ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pai4sk import LogisticRegression\n",
    "# snapml_lr = LogisticRegression(use_gpu=True, device_ids=[0])\n",
    "snapml_lr = LogisticRegression(use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snapml_lr.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model using Snap ML\n",
    "t0 = time.time()\n",
    "model = snapml_lr.fit(X_train, y_train)\n",
    "snapml_time = time.time() - t0\n",
    "print(\"[Snap ML] Training time (s):  {0:.2f}\".format(snapml_time))\n",
    "\n",
    "# Evaluate accuracy on test set\n",
    "snapml_pred = snapml_lr.predict(X_test)\n",
    "print('[Snap ML] Accuracy score : {0:.6f}'.format(accuracy_score(y_test, snapml_pred)))\n",
    "print('[Logistic Regression] Snap ML vs. sklearn speedup : {0:.2f}x '.format(sklearn_time/snapml_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Can you train a Random Forest Classifier with SKLearn and Snap ML and compare the results? \n",
    "\n",
    "Now let's see if you can use the Snap ML API guide to train a Random Forest Classifier and compare it's performance with SKLearn. \n",
    "\n",
    "Snap ML API: https://ibmsoe.github.io/snap-ml-doc/v1.6.0/ranforapidoc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model parameters\n",
    "max_depth     =  10\n",
    "n_estimators  =  50\n",
    "n_jobs        =  16     # e.g. number of threads\n",
    "max_features  =  4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "sklearn_rf = RandomForestClassifier(random_state=0, max_depth=max_depth, n_estimators=n_estimators, n_jobs=n_jobs, max_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a random forest model using scikit-learn\n",
    "t0 = time.time()\n",
    "sklearn_rf.fit(X_train, y_train)\n",
    "sklearn_time = time.time() - t0\n",
    "print(\"[sklearn] Training time (s):  {0:.5f}\".format(sklearn_time))\n",
    "\n",
    "# Evaluate accuracy on test set\n",
    "sklearn_pred = sklearn_rf.predict(X_test)\n",
    "print('[sklearn] Accuracy score : ', accuracy_score(y_test, sklearn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Random Forest model directly from the SnapML package\n",
    "# [insert code here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a random forest model using Snap ML\n",
    "# [insert code here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy on test set\n",
    "# [insert code here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; Copyright IBM Corporation 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
